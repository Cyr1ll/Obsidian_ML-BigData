
## Определение
Обобщённый метрический классификатор – это алгоритм классификации, который основывается на анализе расстояний между классифицируемым объектом $u \in X$ и объектами обучающей выборки $X_\ell$.  

Элементы обучающей выборки $x_1, \dots, x_\ell$ упорядочиваются по возрастанию расстояний до объекта $u$:
$$
\rho(u, x^{(1)}_u) \leq \rho(u, x^{(2)}_u) \leq \dots \leq \rho(u, x^{(\ell)}_u),
$$
где $x^{(i)}_u$ – $i$-й ближайший сосед объекта $u$. Соответственно, метка $i$-го соседа обозначается как $y^{(i)}_u = y^*(x^{(i)}_u)$.

---

## Классификация
Метрический алгоритм классификации относит объект $u$ к тому классу $y \in Y$, для которого **оценка близости** $\Gamma_y(u, X_\ell)$ максимальна:
$$
a(u; X_\ell) = \arg \max_{y \in Y} \Gamma_y(u, X_\ell),
$$
где:
$$
\Gamma_y(u, X_\ell) = \sum_{i=1}^\ell [y^{(i)}_u = y] \cdot w(i, u),
$$
- $w(i, u)$ – весовая функция, которая определяет важность $i$-го соседа для классификации объекта $u$;
- $\Gamma_y(u, X_\ell)$ – оценка близости объекта $u$ к классу $y$.

---

## Весовая функция
Весовая функция $w(i, u)$ обычно выбирается:
- **Неотрицательной**;
- **Не возрастающей** по $i$ (чем ближе объект $x^{(i)}_u$ к $u$, тем больше его вес).  

Примеры весовых функций:
1. **Метод ближайшего соседа (1NN)**:  
   $$ w(i, u) = [i = 1]. $$

2. **Метод $k$ ближайших соседей ($k$NN)**:  
   $$ w(i, u) = [i \leq k]. $$

3. **Метод $k$ взвешенных ближайших соседей**:  
   $$ w(i, u) = [i \leq k] \cdot q_i. $$

4. **Метод Парзеновского окна фиксированной ширины**:  
   $$ w(i, u) = K\left(\frac{\rho(u, x^{(i)}_u)}{h}\right), $$
   где $h$ – ширина окна.

5. **Метод Парзеновского окна переменной ширины**:  
   $$ w(i, u) = K\left(\frac{\rho(u, x^{(i)}_u)}{\rho(u, x^{(k+1)}_u)}\right). $$

6. **Метод потенциальных функций**:  
   $$ w(i, u) = \gamma^{(i)}_u K\left(\frac{\rho(u, x^{(i)}_u)}{h^{(i)}_u}\right). $$

---

## Достоинства
- Простота реализации и возможность модификации через выбор весовой функции $w(i, u)$.
- Интуитивная интерпретация классификации: алгоритм объясняет, почему объект $u$ был отнесён к классу $y$, показывая схожие объекты из обучающей выборки.
- Подходит для областей, где важна интерпретируемость: медицина, биометрия, юриспруденция.

---

## Недостатки
1. **Неэффективное использование памяти**:
   - Алгоритм хранит всю обучающую выборку $X_\ell$, что требует большого объёма памяти.
   - Погрешности в данных или модели расстояния $\rho$ могут снижать точность классификации на границе классов.

2. **Высокая вычислительная сложность**:
   - Для поиска ближайшего соседа требуется $O(\ell)$ операций.
   - Проблема решается с помощью эффективных методов поиска ближайших соседей ($O(\ln \ell)$).

3. **Ограниченная параметризация**:
   - Простейшие версии (например, $k$NN) имеют мало параметров, что затрудняет настройку алгоритма под конкретные данные.

---

## Примечания
Метрические алгоритмы относятся к методам рассуждения по прецедентам (case-based reasoning, CBR). Они используют гипотезу компактности, предполагая, что близкие объекты с большей вероятностью принадлежат одному классу.  

Оптимизация алгоритма часто включает выбор минимального подмножества эталонных объектов, действительно необходимых для классификации.
