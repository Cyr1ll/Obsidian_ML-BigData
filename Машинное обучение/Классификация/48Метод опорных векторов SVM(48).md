# Метод опорных векторов (SVM)
#SVM
## Введение
Метод опорных векторов (Support Vector Machine, SVM) — это мощный алгоритм машинного обучения, применяемый для задач классификации, регрессии и обнаружения выбросов. Основная идея метода заключается в нахождении оптимальной гиперплоскости, которая разделяет классы в пространстве признаков с максимальным зазором (margin).

---

## Основная идея
- **Цель метода**: найти такую гиперплоскость, которая разделяет данные двух классов с наибольшим запасом (максимальным margin).
- ![[Pasted image 20250117121324.png]]
- **Опорные векторы**: это объекты, которые находятся ближе всего к разделяющей гиперплоскости. Они определяют положение и ориентацию гиперплоскости.

Если данные линейно разделимы, гиперплоскость задается уравнением:
$$
w \cdot x + b = 0,
$$
где:
- $w$ — вектор весов, перпендикулярный гиперплоскости,
- $b$ — смещение гиперплоскости,
- $x$ — вектор признаков.

---

## Формулировка задачи

### 1. Линейно разделимая задача
Для линейно разделимых данных задача состоит в нахождении гиперплоскости, удовлетворяющей условиям:
$$
y_i (w \cdot x_i + b) \geq 1, \quad \forall i,
$$
где:
- $y_i \in \{-1, 1\}$ — метка класса объекта $x_i$.

**Оптимизационная задача**: минимизировать $||w||^2 / 2$ (квадрат нормы весов $w$), что соответствует максимизации зазора между классами:
$$
\min_{w, b} \frac{1}{2} ||w||^2, \quad \text{при условии } y_i (w \cdot x_i + b) \geq 1.
$$

### 2. Линейно неразделимая задача (с допущением ошибок)
Если данные линейно неразделимы, вводятся **переменные допущений ошибок** $\xi_i$:
$$
y_i (w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0.
$$

Обновленная оптимизационная задача:
$$
\min_{w, b, \xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^n \xi_i,
$$
где:
- $C$ — коэффициент регуляризации, который контролирует баланс между максимизацией зазора и штрафом за ошибки классификации.

### 3. Нелинейная классификация (ядровый метод)
Для нелинейных задач используется **ядровый трюк**. Данные переносятся в пространство большей размерности с помощью функции $\phi(x)$, а затем метод решается как для линейного случая:
$$
K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j),
$$
где $K(x_i, x_j)$ — ядровая функция.

[[Я49дерные функции методы опорных векторов(49)]]:
- **Линейное ядро**: $K(x_i, x_j) = x_i \cdot x_j$.
- **Полиномиальное ядро**: $K(x_i, x_j) = (x_i \cdot x_j + c)^d$.
- **Радиально-базисное ядро (RBF)**: $K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$.
- **Сигмоидное ядро**: $K(x_i, x_j) = \tanh(\alpha x_i \cdot x_j + c)$.

---

## Алгоритм работы метода SVM
1. **Подготовка данных**:
   - Приведение данных к единым масштабам (например, нормализация или стандартизация).
2. **Выбор ядра**:
   - Для линейной задачи используется линейное ядро, для нелинейных — полиномиальное или RBF.
3. **Оптимизация**:
   - Решение задачи минимизации с использованием методов оптимизации, таких как метод множителей Лагранжа.
4. **Определение опорных векторов**:
   - Выбор объектов, которые лежат на границе разделяющих классы (или близки к ней).
5. **Прогнозирование**:
   - Для нового объекта $x$ знак функции $f(x)$ определяет его класс:
     $$
     f(x) = \text{sign}(w \cdot x + b).
     $$

---

## Преимущества метода
1. **Гибкость**: SVM работает как с линейными, так и с нелинейными задачами (благодаря ядровому методу).
2. **Эффективность**: подходит для задач с большим числом признаков.
3. **Контроль над ошибками**: параметр $C$ позволяет настраивать модель в зависимости от задачи.
4. **Оптимальная гиперплоскость**: минимизирует ошибку и увеличивает зазор.

---

## Недостатки метода
1. **Чувствительность к выбору ядра и параметров** (например, ширины окна в RBF).
2. **Высокая вычислительная сложность** на больших наборах данных.
3. **Плохая интерпретируемость**: модель сложнее понять, особенно в случае нелинейных ядер.

---

## Пример
Пусть у нас есть два класса: $C_1$ и $C_2$. Мы хотим найти оптимальную гиперплоскость, разделяющую их.

1. **Шаг 1**: Рассчитываем расстояния от объектов до гиперплоскости.
2. **Шаг 2**: Определяем опорные векторы (объекты, которые лежат на границе разделения).
3. **Шаг 3**: Находим параметры $w$ и $b$ для гиперплоскости, которая максимизирует зазор.

---

## Полезные ссылки
- [[Я49дерные функции методы опорных векторов(49)]]
- https://habr.com/ru/articles/802185/
- https://habr.com/ru/articles/428503/

