# Дерево решений и понятие однородности  

## Основная идея  
Дерево решений – это **непараметрический метод машинного обучения с учителем**, который используется для решения задач классификации и регрессии.  
Примеры применения: поисковые системы, рекомендательные системы.  
**![|500](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcAichlIqlSF54Vh1pvE38TOvEH8chb7bJbB7E9dTO5nOzxiSJq-zgAiabgGGA1Uj3cXuYAZ07zANwiZsVqoKhmUwiZkg-0M8UDA8tW7monnxhViNsKb_WSolS1MW9KryXkv4dK5L7nz1czPsFXI4eMMav1?key=C1AMzwFdvttb7H1YBkknyQ)**
---

## Основные элементы дерева решений  
1. **Корень дерева** – верхний узел, с которого начинается построение дерева.  
2. **Узел** – точка принятия решения, содержащая пару "признак и пороговое значение".  
3. **Лист** – узел, не имеющий дочерних узлов. Представляет конечное решение. Листовые узлы могут быть:  
   - **Чистыми** – содержат образцы одного класса.  
   - **Нечистыми** – содержат смесь классов.  
4. **Путь** – последовательность правил от корня до узла.  
5. **Ветвь** – соединение между узлами, представляющее правило разделения.  

---

## Работа дерева решений  
- **Классификация**: если целевая переменная принимает дискретные значения.  
- **Регрессия**: если целевая переменная принимает вещественные значения.  

Деревья решений строятся на основе алгоритмического подхода, который разделяет данные на подмножества, исходя из заданных условий.  

---

## Преимущества  
- **Простота и интерпретируемость**: модель легко объяснить и визуализировать.  
- **Отсутствие требований к нормализации данных**: не требуется масштабирование или преобразование данных.  
- **Поддержка категориальных данных**: в отличие от логистической регрессии или SVM, деревья решений не требуют кодирования данных в фиктивные переменные.  

## Ограничения  
- **Проблемы с обобщением**: склонны к переобучению, особенно при работе с небольшими или несбалансированными выборками.  
- **Смещение**: если обучающая выборка несбалансирована, дерево может давать смещённые предсказания.  

---

## Методы разделения выборки  
### Алгоритмы  
1. **CART**: использует индекс Джини.  
2. **ID3**: использует энтропию.  

### Метрики разделения  
1. **Энтропия**  
   - Мера неопределённости или беспорядка в выборке.  
   - Используется для уменьшения неопределённости при создании дерева.  

   Формула:  
   $$  
   H(S) = - \sum_{i=1}^n P_i \log_2(P_i),  
   $$  
   где $P_i$ – вероятность принадлежности к классу $i$.  

2. **Индекс Джини**  
   - Мера неоднородности выборки.  
   - Низкий индекс Джини соответствует более однородной выборке, а высокий – более неоднородной.  

   Формула:  
   $$  
   G(S) = 1 - \sum_{i=1}^n P_i^2,  
   $$  
   где $P_i$ – вероятность принадлежности к классу $i$.  

   **Примечание**: индекс Джини часто используется как более оптимальная метрика по сравнению с энтропией.  

---

## Пример использования дерева решений  
Для классификации клиента на основе его данных о покупках.  
1. Корень дерева – анализ ключевого признака, например, возраста.  
2. Узлы – проверка дополнительных условий, таких как доход или предпочтения.  
3. Лист – окончательное решение, например, "вероятность покупки".  
