# Случайный лес, Bagging, Stacking  

## Случайный лес (Random Forest)  

### Основная идея  
Случайный лес — это ансамблевый метод машинного обучения, который строит множество решающих деревьев.  
Каждое дерево обучается на случайной подвыборке данных (*bootstrap*), а итоговый результат формируется путём агрегирования предсказаний всех деревьев:  
- **Для классификации**: используется голосование по большинству.  
- **Для регрессии**: берётся среднее значение предсказаний.  
- **![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfY_UFq4pX07AQQdYr3o1EOZ26PwX6EZZW2aRhmmv_Xe2BmnZVj62zohih2cplD3MFafPhJdWUy-QeCz_Zt9k29DcXWEYgAw9Kdzx5VB8pVQLRY1lbVC9HQsKXpwTQ93W39W3XRu7Cn-dWbInXNIXaGuxc?key=C1AMzwFdvttb7H1YBkknyQ)**

### Особенности алгоритма  
1. В каждом дереве на каждом разбиении выбирается случайное подмножество признаков.  
2. Это уменьшает корреляцию между деревьями и улучшает обобщающую способность ансамбля.  
3. Элементы обучающей выборки, которые не попали в *bootstrap*, используются для оценки производительности (*out-of-bag score*).  

### Преимущества  
- Высокая точность и стабильность.  
- Работает с разными типами признаков.  
- Устойчив к пропущенным значениям и выбросам.  

### Недостатки  
- Высокая вычислительная сложность для больших наборов данных.  
- Возможность переобучения при избыточном числе деревьев.  
- Сложность интерпретации моделей.  

---

## Bagging  

### Основная идея  
Бэггинг (*bootstrap aggregating*) — метод уменьшения дисперсии модели и улучшения её устойчивости.  
Суть: обучить несколько моделей на различных бутстреп-выборках и усреднить их предсказания.  
**![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcW56XwDL9zZZWaMhDV8gAwazXURwDA7L9oQvNf-nLqreTLj6QNc7r7AWqemzxHN2GARIgmNJyBnXmuByYrJP-MDdLz1bkon2X_XXJFG4vbX9GqGcgYVlf81cXef9SZ0zHF6n43DdJ4gbyrcl1haqzTtvdI?key=C1AMzwFdvttb7H1YBkknyQ)**

### Алгоритм  
1. **Создать бутстреп-выборки**: из исходного набора случайно выбрать $n$ элементов с возвращением.  
2. **Обучить модели**: на каждой выборке обучить отдельный классификатор (например, дерево решений).  
3. **Агрегация результатов**:  
   - Для классификации: голосование по большинству.  
   - Для регрессии: усреднение предсказаний.  

### Преимущества  
- Снижает дисперсию модели.  
- Уменьшает риск переобучения.  
- Прост в реализации.  

### Недостатки  
- Требует вычислительных ресурсов при большом числе моделей.  
- Модели должны быть склонны к переобучению для максимального эффекта.  

---

## Stacking  

### Основная идея  
Стекинг — ансамблевый метод, объединяющий предсказания различных моделей с помощью обучаемой метамодели.  
В отличие от бэггинга и случайного леса, стекинг позволяет использовать алгоритмы разного типа.  

### Алгоритм  
1. **Разделение данных**: тренировочная выборка делится на $k$ фолдов (как в кросс-валидации).  
2. **Предсказания базовых моделей**:  
   - Каждая модель обучается на $(k-1)$ фолдах.  
   - На $k$-м фолде модель делает предсказания, формируя мета-признаки.  
   - **![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeqIRYnXSf6pNAWc-BC_b_MSLw8Ik-CeqGAxtvQIxiZNcjI-F3756TMsn5zM65onub7UymNlHnR98mt9Y8z_buT-FiO62_P2tVJG4ZWAIBCO0Xvr-gdvEZfF8YcKb0o95QXGgg9Dz_BU4iYFWAA_4_SVImF?key=C1AMzwFdvttb7H1YBkknyQ)**
1. **Обучение метамодели**:  
   - Мета-признаки используются для обучения метамодели.  
   - Дополнительно метамодель может использовать исходные признаки.  
   - **![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVxxZLrd0gMvoN27f3a_U6-hhQ_2wHQpPcKKWhYobpdVHB9RSl2io0y3IQiqtGBiTl6ftDNlpjDxHxkN4Hac9g-hiRoafxI93V4LVhWSCx623alZqbSK-wLTuILPg6SwUlPl2etp370EoaBQbGmvw4qUUz?key=C1AMzwFdvttb7H1YBkknyQ)**

**![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe-HlaIUgn5fgiprCTGCDcagdEOMwuHOlD0gteThZtjjjtIhZ50LhooF7yq5YSfzigFmAYovtgqVHdTpIOSa5qc_fTajBJQ4Qyc5oaoVNQuivmDUInS2uLNh2wCElyC_2_me9zLLt0HzeRdK3G-7OZ4Xsk?key=C1AMzwFdvttb7H1YBkknyQ)**
### Преимущества  
- Гибкость: можно комбинировать модели разных типов.  
- Высокая точность за счёт комбинирования сильных сторон разных моделей.  

### Недостатки  
- Сложность настройки: необходимо выбирать архитектуру и параметры базовых моделей и метамодели.  
- Длительное время обучения из-за многоуровневой структуры.  
- Возможность переобучения метамодели.  


### Отличия Random Forest от бэггинга

Random Forest использует бэггинг как основу, но добавляет дополнительные механизмы для снижения корреляции между деревьями и повышения обобщающей способности:

1. **Метод случайных подпространств (Random Subspaces)**:
    
    - На каждом разбиении дерева выбирается случайное подмножество признаков из доступных.
    - Это уменьшает влияние наиболее информативных признаков, что делает деревья более независимыми.
    - Размер подмножества задаётся гиперпараметром max_features\text{max\_features}max_features, который выбирается пользователем.
2. **Декорреляция деревьев**:
    
    - В бэггинге все деревья используют одинаковый набор признаков, что может привести к высокой корреляции между деревьями.
    - В Random Forest за счёт случайного выбора признаков на каждом узле деревья становятся менее похожими друг на друга.
3. **Упрощение модели**:
    
    - В Random Forest каждое дерево растёт до максимальной глубины без необходимости обрезки (_pruning_), что упрощает обучение отдельных деревьев.
    - При этом итоговый ансамбль компенсирует склонность отдельных деревьев к переобучению.
4. **Out-of-Bag (OOB) оценка**:
    
    - В бэггинге для оценки качества модели обычно используется тестовая выборка.
    - Random Forest позволяет использовать элементы обучающей выборки, которые не попали в _bootstrap_ (Out-of-Bag) для оценки производительности, исключая необходимость отдельной валидационной выборки.

Эти дополнительные шаги делают Random Forest более эффективным и универсальным инструментом по сравнению с классическим бэггингом.