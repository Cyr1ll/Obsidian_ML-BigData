# Алгоритм ID3  

## Основная идея  
ID3 (Iterative Dichotomiser 3) – это алгоритм обучения деревьев решений, который используется для задач классификации. Он основывается на принципе рекурсивного разбиения обучающего множества на подмножества с использованием решающих правил, минимизирующих энтропию.  

---

## Ключевые концепции  

### Прирост информации (Information Gain)  
Для выбора атрибута на каждом шаге используется **прирост информации (IG)** – мера, показывающая, насколько атрибут снижает неопределённость в данных:  
$$
IG = E_{\text{parent}} - E_{\text{child}},
$$  
где:  
- $E_{\text{parent}}$ – энтропия родительского узла,  
- $E_{\text{child}}$ – взвешенная сумма энтропий дочерних узлов.  

**Формула для расчёта IG**:  
$$
IG(y, x) = E(y) - E(y|x),
$$  
где:  
- $E(y)$ – энтропия исходного множества,  
- $E(y|x)$ – условная энтропия после разделения по атрибуту $x$.  

### Энтропия  
Энтропия измеряет степень неопределённости (или "беспорядка") в выборке:  
$$
E(S) = - \sum_{i=1}^n P_i \log_2(P_i),
$$  
где $P_i$ – вероятность принадлежности объекта классу $i$.  

---

## Алгоритм работы ID3  
1. Вычислить энтропию для текущего узла.  
2. Для каждого атрибута вычислить прирост информации.  
3. Выбрать атрибут с максимальным приростом информации для разбиения узла.  
4. Разделить обучающее множество на подмножества по значению выбранного атрибута.  
5. Повторять процесс рекурсивно для каждого дочернего узла до выполнения условия остановки.  

---

## Методы предотвращения переобучения  

1. **Ранняя остановка**:  
   - Ограничение глубины дерева.  
   - Минимальное количество элементов в узле.  
   - Условие на уменьшение "примесей" (например, минимизация энтропии).  

2. **Обрезка дерева**:  
   - Удаление элементов дерева, вызывающих переобучение.  
   - Методы обрезки:  
     - Сокращение смежности ошибок (reduced error pruning).  
     - Сокращение критических значений.  

3. **Сокращение сложности затрат**:  
   - Добавление штрафа за сложность модели с использованием гиперпараметра $\alpha$.  
   - Общая функция потерь:  
     $$  
     Loss(T) = MSE(T) + \alpha \cdot |T|,  
     $$  
     где $|T|$ – число узлов в дереве.  
   - Если $\alpha = 0$, минимизируются потери на обучении (дерево совпадает с исходным).  
   - Если $\alpha > 0$, добавляется штраф за количество конечных узлов.  
   - **![|200](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1rMWQURmTvCQMgKs3PCIpvxvoYewTWwvYf0C3AWqbtZujXHGghVhhzr7ednk5huFhFkLNeR63byZyD0oUeVIc3hl5j1b9lyWNdl5tDM3KWHhXFpdI-9xLv4BK6Zxe4UlbTHTL9l4mYJPSrEoK6ouyeGxT?key=C1AMzwFdvttb7H1YBkknyQ)**

4. **Предобработка данных**:  
   - Изменение исходной выборки для уменьшения сложности задачи и устранения шума.  

---

## Преимущества ID3  
- Простота реализации и интерпретации.  
- Использование информационного подхода для выбора атрибутов.  

## Недостатки ID3  
- Склонность к переобучению, особенно при большом числе признаков или классов.  
- Неспособность работать с непрерывными атрибутами (это ограничение исправлено в алгоритме C4.5).  
- Чувствительность к несбалансированным данным.  

## Пример
**![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXemPHmjyrjC5eWZcjKvqMNVDEpj-INLz3OKsV9nausIQasg19nf26RXgYTh5jT8k-HeViwIQdh4HVWZEi8YfIT4BulFFdGOlur1ltHscHNJ1Y4QGuLlGYSPduQIwHv9WD8g19DGRKy1JRnh-dCsPmUEqjuC?key=C1AMzwFdvttb7H1YBkknyQ)**
**![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf00Ulg9RrOm7tVuUqj50aR7wY6qjdd929kYhcLOkquYggDktlyWeXrxKvYoOyvAJbm5JUKR2x_IT_Ph2l-vrMlV24SjPQw_0w0_AFO_IwZdXL1Pf9f35a2e9ELSxyHwbYauChXLhmTP-O0u6vvDV2NSCQ?key=C1AMzwFdvttb7H1YBkknyQ)**