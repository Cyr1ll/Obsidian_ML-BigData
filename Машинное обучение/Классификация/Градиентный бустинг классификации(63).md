# Градиентный бустинг для классификации

## Введение
**Градиентный бустинг** — это контролируемый ансамблевый метод машинного обучения, который используется для построения сильной модели путем объединения слабых классификаторов. Основная идея метода заключается в минимизации заданной функции потерь за счет итеративного добавления новых моделей, которые исправляют ошибки предыдущих.

---

## Основная идея
Метод градиентного бустинга работает на основе:
- Итеративного обучения слабых моделей (например, деревьев решений).
- Минимизации ошибки за счет использования градиента функции потерь.
- Поддержки произвольных дифференцируемых функций потерь, что делает метод гибким.

---

## Основные шаги алгоритма
1. **Инициализация:**  
   Начинаем с базовой модели, которая минимизирует функцию потерь:
   $$
   F_0(x) = \arg \min_c \sum_{i=1}^N L(y_i, c)
   $$
   где $L$ — функция потерь, а $y_i$ — истинные метки.

2. **Итерации для $m = 1, 2, \dots, M$:**
   - Вычисляем псевдоостатки (градиенты функции потерь):
     $$
     r_i^{(m)} = -\frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}
     $$
   - Обучаем слабую модель $h_m(x)$ для предсказания остатков $r_i^{(m)}$.
   - Обновляем итоговую модель:
     $$
     F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
     $$
     где $\nu$ — скорость обучения (learning rate).

3. **Финальное решение:**  
   Предсказание вероятности принадлежности к классу:
   $$
   P(y = k \mid x) = \frac{\exp(F_M^k(x))}{\sum_{j=1}^K \exp(F_M^j(x))}
   $$

---

## Преимущества
- **Поддержка произвольных дифференцируемых функций потерь:**  
  Это позволяет использовать метод для различных задач, включая классификацию, ранжирование и др.
- **Выдает вероятности:**  
  Результаты представлены в виде вероятностей принадлежности к классам, что удобно для интерпретации.
- **Работа с пропущенными данными:**  
  Эффективно справляется с отсутствующими значениями в данных.

---

## Недостатки
- **Чувствительность к шуму:**  
  На данных с большим количеством шумов метод может показывать нестабильные результаты.
- **Склонность к переобучению:**  
  При использовании большого количества деревьев и отсутствии регуляризации модель может переобучиться.
- **Высокая вычислительная сложность:**  
  Обучение требует значительных вычислительных ресурсов и времени. Метод сложно распараллелить из-за итеративного характера.

---
**![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc-SrgtLW_M3yxWwOhwFrKfQgLTFOpFZvcTJkdkxbwePDo3XdQe8T5Kpu_gDmzgYextpYY6xcjYKzei_BvzY-hlpiAxxN0ka4AtfvKfeBmCg04CKyHioabAkDHykY56306NDSmH_umsEtU5TfBMe3GutKcX?key=C1AMzwFdvttb7H1YBkknyQ)**
## Полезные ссылки
- [Градиентный бустинг на Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)
- [Основы бустинга в машинном обучении](https://education.yandex.ru/handbook/ml/article/boosting)
- [XGBoost Documentation](https://xgboost.readthedocs.io)
