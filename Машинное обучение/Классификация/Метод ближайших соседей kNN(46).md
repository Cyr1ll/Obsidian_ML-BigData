# Метод ближайших соседей (kNN)

## Введение
**Метод k ближайших соседей (k-Nearest Neighbors, kNN)** — это один из простейших методов классификации и регрессии, основанный на предположении, что объекты из одного класса находятся близко друг к другу в пространстве признаков.

---

## Основная идея
Для классификации или предсказания значения нового объекта алгоритм:
1. Сравнивает объект с обучающими данными.
2. Находит $k$ ближайших соседей (объектов из обучающей выборки).
3. Принимает решение, основываясь на классах или значениях этих соседей.

---

## Алгоритм работы
![[Pasted image 20250117120558.png]]
1. **Задать гиперпараметр $k$**: Определяем количество ближайших соседей, которые будут учитываться при принятии решения.
2. **Вычислить расстояния**: Для классифицируемого объекта вычисляем расстояние до каждого объекта обучающей выборки (например, евклидово расстояние).
   - Евклидово расстояние:
     $$
     d(x, x_i) = \sqrt{\sum_{j=1}^n (x_j - x_{ij})^2},
     $$
     где:
     - $x = (x_1, x_2, ..., x_n)$ — вектор признаков классифицируемого объекта,
     - $x_i = (x_{i1}, x_{i2}, ..., x_{in})$ — вектор признаков $i$-го объекта обучающей выборки,
     - $n$ — размерность пространства признаков.
3. **Отсортировать объекты обучающей выборки** по возрастанию расстояния до классифицируемого объекта.
4. **Выбрать $k$ ближайших соседей**: Из обучающей выборки берём $k$ объектов с минимальными расстояниями.
5. **Принять решение**:
   - **Классификация**:
     - Выбирается класс, к которому принадлежит большинство из $k$ ближайших соседей (правило большинства).
     - Взвешенный вариант: класс определяется с учётом расстояний (более близким соседям даётся больший вес).
   - **Регрессия**:
     - Предсказываемое значение вычисляется как среднее арифметическое (или взвешенное среднее) значений $k$ ближайших соседей.
     $$
     \hat{y} = \frac{1}{k} \sum_{i=1}^k y_i,
     $$
     где $y_i$ — значения ближайших соседей.

---

## Пример для классификации
Пусть $k = 3$, а классы ближайших соседей: $[A, B, A]$.
- **Результат**: Классифицируемый объект относится к классу $A$ (большинство соседей принадлежат этому классу).
- ![[Pasted image 20250117120506.png]]

---

## Гиперпараметр $k$
- **Малое $k$** (например, $k = 1$):
  - Алгоритм становится чувствительным к шуму.
  - Модель может "переобучиться" на обучающие данные.
- **Большое $k$**:
  - Увеличивает устойчивость к шуму.
  - Уменьшает влияние ближайших соседей, что может ухудшить качество классификации.

---

## Метрики расстояния
Выбор метрики расстояния влияет на результаты работы метода. Наиболее популярные:
1. **Евклидово расстояние**:
   $$
   d(x, x_i) = \sqrt{\sum_{j=1}^n (x_j - x_{ij})^2}.
   $$
2. **Манхэттенское расстояние**:
   $$
   d(x, x_i) = \sum_{j=1}^n |x_j - x_{ij}|.
   $$
3. **Косинусное расстояние** (используется для текстов):
   $$
   d(x, x_i) = 1 - \frac{\sum_{j=1}^n x_j x_{ij}}{\sqrt{\sum_{j=1}^n x_j^2} \cdot \sqrt{\sum_{j=1}^n x_{ij}^2}}.
   $$
4. **Хэммингово расстояние** (для категориальных данных):
   - Количество позиций, в которых строки различаются.

---

## Преимущества метода
1. Простота в реализации и понимании.
2. Не требует обучения: все вычисления выполняются на этапе классификации или предсказания.
3. Универсальность: применим как для классификации, так и для регрессии.

---

## Недостатки метода
1. **Высокая вычислительная сложность**:
   - Для больших выборок и данных высокой размерности поиск ближайших соседей становится ресурсоёмким.
2. **Выбор параметров**:
   - Неочевидный выбор $k$ и метрики расстояния.
3. **Чувствительность к масштабу признаков**:
   - Перед использованием метода важно нормализовать данные.
4. **Неустойчивость к шуму**:
   - Малое $k$ делает метод чувствительным к выбросам.

---

## Ускорение метода
1. **Использование эффективных структур данных**:
   - K-d деревья.
   - Ball-деревья.
2. **Понижение размерности**:
   - Метод главных компонент (PCA).
   - t-SNE для визуализации.

---

## Заключение
Метод $k$ ближайших соседей — это простой, но мощный инструмент для задач классификации и регрессии. Однако его применение требует учёта масштабов данных, выбора подходящей метрики расстояния и значения $k$. Для больших наборов данных желательно использовать оптимизированные алгоритмы поиска ближайших соседей.

---

## Полезные ссылки
- [[Парзеновские окна(26)]]
- [[Постановка задачи классификации(30_14&2)]]
- https://habr.com/ru/articles/801885/
