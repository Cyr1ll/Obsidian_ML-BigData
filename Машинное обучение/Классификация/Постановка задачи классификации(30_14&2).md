# Постановка задачи классификации

## Основные обозначения
Для постановки задачи классификации используются следующие обозначения:
- $X$ – пространство объектов (признаковое пространство).
- $Y$ – конечное множество имён классов.
- $\rho: X \times X \to [0, \infty)$ – функция расстояния на пространстве $X$.

Существует целевая зависимость $y^*: X \to Y$, которая задаёт правильные метки классов для объектов. Однако значения $y^*$ известны **только** на объектах обучающей выборки:
$$
X_\ell = \{(x_i, y_i)\}_{i=1}^\ell, \quad y_i = y^*(x_i),
$$
где:
- $x_i \in X$ – объект обучающей выборки;
- $y_i \in Y$ – метка класса объекта $x_i$.

---

## Задача
Требуется построить алгоритм классификации $a: X \to Y$, который аппроксимирует целевую зависимость $y^*(x)$ на всём множестве $X$.

Алгоритм классификации должен:
1. Для произвольного объекта $u \in X$ предсказывать его метку класса $a(u) \in Y$.
2. Минимизировать ошибки классификации, то есть обеспечить, чтобы:
   $$
   a(x) \approx y^*(x), \quad \forall x \in X.
   $$

---

## Классификация как задача обучения
Классификацию можно рассматривать как задачу обучения с учителем:
- **Входные данные**: обучающая выборка $X_\ell$.
- **Выходные данные**: построенный алгоритм $a$, который может классифицировать новые объекты.

Цель классификации – построить алгоритм, который обладает:
1. **Обобщающей способностью** – высокая точность классификации на новых данных, не входящих в обучающую выборку.
2. **Робастностью** – устойчивость к шуму в данных и возможным аномалиям.

---

## Метрические алгоритмы классификации
Метрические алгоритмы классификации являются одной из ключевых методологий для решения задачи классификации. Их основные характеристики:
1. **Использование функции расстояния $\rho$**:
   - Для определения близости объектов.
   - Для выявления схожих объектов в обучающей выборке.

2. **Принцип ближайших соседей**:
   - Классификация нового объекта $u$ выполняется на основе анализа его соседей в обучающей выборке $X_\ell$.

3. **Прецедентная логика**:
   - Решение задачи классификации основывается на известных прецедентах (объектах обучающей выборки), что делает подход интерпретируемым.

---

## Пример метрического алгоритма: $k$ ближайших соседей ($k$NN)
1. Для классифицируемого объекта $u \in X$ вычисляются расстояния до всех объектов обучающей выборки $x_i \in X_\ell$.
2. Выбираются $k$ ближайших соседей объекта $u$ (в зависимости от функции расстояния $\rho$).
3. Итоговая метка объекта $u$ определяется голосованием среди меток $k$ ближайших соседей:
   $$
   a(u) = \arg \max_{y \in Y} \sum_{i=1}^k [y_i = y].
   $$

---

## Примечания
1. **Роль обучающей выборки**:
   - Обучающая выборка $X_\ell$ выступает в роли эталона, на основе которого принимаются решения для новых объектов.
   - Метрические алгоритмы требуют хранения всей выборки для работы, что может быть неэффективно при больших объёмах данных.

2. **Настройка параметров**:
   - Для метрических алгоритмов важно правильно выбрать:
     - Функцию расстояния $\rho$ (например, Евклидова, Косинусная, Манхэттенская).
     - Гиперпараметры (например, число соседей $k$ для $k$NN).

3. **Обобщение на другие алгоритмы**:
   - Помимо $k$NN, метрические подходы включают методы Парзеновского окна, потенциальных функций и другие модификации.
