# Градиентный бустинг для регрессии

## Введение
**Градиентный бустинг** — это контролируемый ансамблевый метод машинного обучения, используемый для нахождения нелинейной зависимости между признаками и целевыми переменными. Этот подход подходит как для задач регрессии, так и для классификации.

---

## Основная идея
Метод основан на итеративном объединении и дополнении слабых моделей (обычно деревьев решений) в итоговую модель, чтобы минимизировать заданную функцию потерь. На каждом этапе добавляется новая модель, которая корректирует ошибки предыдущих моделей, основываясь на градиенте функции потерь.

---

## Особенности
- **Подходит для работы с шумными данными:** Градиентный бустинг хорошо справляется с пропущенными значениями и категориальными признаками.
- **Универсальность:** Метод эффективен как для регрессии, так и для классификации.
- **Гибкость:** Поддерживает различные функции потерь, такие как MSE для регрессии и кросс-энтропия для классификации.

---

## Основные шаги алгоритма
1. **Инициализация:**  
   Начинаем с простой модели, например, константного значения, которое минимизирует функцию потерь:
   $$
   F_0(x) = \arg \min_c \sum_{i=1}^N L(y_i, c)
   $$
   где $L$ — функция потерь, а $y_i$ — истинные значения.

2. **Итерации для $m = 1, 2, \dots, M$:**
   - Вычисляем псевдоостатки (градиент функции потерь):
     $$
     r_i^{(m)} = -\frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}
     $$
   - Обучаем слабую модель $h_m(x)$ для предсказания остатков $r_i^{(m)}$.
   - Обновляем итоговую модель:
     $$
     F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
     $$
     где $\nu$ — скорость обучения (learning rate).

3. **Финальное решение:**  
   Итоговая модель:
   $$
   F_M(x) = \sum_{m=1}^M \nu \cdot h_m(x)
   $$

---

## Преимущества
- **Высокая точность:** Градиентный бустинг показывает хорошие результаты на сложных задачах.
- **Устойчивость к выбросам:** Может обрабатывать сложные данные с шумами и пропусками.
- **Масштабируемость:** Современные реализации, такие как XGBoost, LightGBM и CatBoost, позволяют работать с большими наборами данных.

---

## Ограничения
- **Чувствительность к переобучению:** Если не контролировать параметры, модель может переобучиться.
- **Высокая вычислительная сложность:** Из-за итеративной природы требуется больше времени на обучение по сравнению с другими методами.

---
**![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfAJEDj1NtsJV-k-zQFTB0Pkg63w2XjxUmA09Ky_Gx01iqY47a5M1249YMFspVm2yromrYFo1nAJNIIL8KAAhjEAsT2IEGnQq9Db1AMELkJPkVhJT_SDoMsrxY0goCfI5D8gFDdzRJ3yj2YxtonjZF3z2Lv?key=C1AMzwFdvttb7H1YBkknyQ)**
## Полезные ссылки
- [XGBoost Documentation](https://xgboost.readthedocs.io)
- [Раздел про бустинг в Yandex.Handbook](https://education.yandex.ru/handbook/ml/article/boosting)
- Хихихихихи - https://habr.com/ru/articles/799725/
