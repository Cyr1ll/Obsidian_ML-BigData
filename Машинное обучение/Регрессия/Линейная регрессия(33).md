#Линейная_регрессия

## Определение
**Линейная регрессия** — это регрессионная модель, описывающая зависимость одной (зависимой, объясняемой) переменной $y$ от одной или нескольких независимых переменных (факторов, регрессоров) $x$ с использованием линейной функции.

Регрессия — это зависимость $E(y|x) = f(x)$ математического ожидания некоторой случайной величины $y$ (зависимой переменной) от одной или нескольких других величин $x$ (независимых переменных). Задача регрессионного анализа заключается в поиске такой функции $f$, которая описывает эту зависимость.

Регрессия помогает выбрать из множества функций ту, которая минимизирует функцию потерь, отражающую степень отклонения модели от экспериментальных данных.

---

## Основная модель
Линейная регрессия описывается следующим уравнением:
$$
y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n + \varepsilon,
$$
где:
- $y$ — зависимая переменная (выход),
- $x_1, x_2, \dots, x_n$ — независимые переменные (регрессоры, входы),
- $w_0, w_1, \dots, w_n$ — параметры модели (веса),
- $\varepsilon$ — случайная ошибка (шум), характеризующая отклонения данных от модели.

Для упрощения записи вводят векторное представление:
$$
y = Xw + \varepsilon,
$$
где:
- $X$ — матрица признаков (объекты и их значения признаков),
- $w$ — вектор параметров модели,
- $\varepsilon$ — вектор ошибок.

---

## Метод наименьших квадратов (МНК)
Для нахождения параметров модели $w$ используется критерий минимизации квадратичной функции потерь:
$$
L(w) = \sum_{i=1}^m \left( y_i - \hat{y}_i \right)^2,
$$
где:
- $y_i$ — истинные значения зависимой переменной,
- $\hat{y}_i$ — предсказанные моделью значения.

В матричной форме:
$$
L(w) = \|y - Xw\|^2.
$$

**Решение задачи минимизации:**
$$
w = (X^T X)^{-1} X^T y,
$$
где:
- $X^T$ — транспонированная матрица признаков,
- $(X^T X)^{-1}$ — обратная матрица к $X^T X$ (требует невырожденности $X^T X$).

---

## Преимущества линейной регрессии
1. Простота реализации и интерпретации результатов.
2. Возможность вычисления аналитического решения.
3. Хорошо работает при линейной зависимости между признаками и целевой переменной.

---

## Ограничения линейной регрессии
1. **Линейность:** модель предполагает линейную зависимость, что может быть ограничением в реальных данных.
2. **Мультиколлинеарность:** высокая корреляция между признаками может негативно повлиять на устойчивость решения.
3. **Шум и выбросы:** чувствительна к выбросам в данных.

---

## Вывод
Линейная регрессия — это базовый метод в задачах регрессии. Он подходит для данных с линейной зависимостью, но требует внимательного выбора признаков и учета ограничений модели.
